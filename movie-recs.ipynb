{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Simple Movie Recommender\n",
    "\n",
    "Create a basic item-to-item K-nearest neighbors collaboritive filtering solution for recommending movies. This builds on a matrix notation  built from the Verstrepen2017 matrix notation of a score matrix factored by user-to-user similarity and the ratings matrix.\n",
    "\n",
    "Score =  Ratings x Similarity\n",
    "\n",
    "where \"Ratings\" is a UxI matrix of user ratings, U is the number of users and I is the number of rated items, \"Similarity\" is an IxI matrix of item-to-item similarity measures.  In this notebook we will use the cosine similarity which measures the angle between the length normalized.\n",
    "\n",
    "The dot product between the Ratings and Similarity produces a UxI Score matrix that contains a score for each user-item pair.  We can sort a user's scores and then recommend the top-n highest scoring items.\n",
    "\n",
    "We are going to create simple movie recommender using the [MovieLens data set](https://grouplens.org/datasets/movielens/), a popular research dataset in the recommender systems domain.  A common approach for movie recommendations is to use known user ratings to predict how a user would rate the movies for which the user has no ratings.  The hightest rated movies are then recommended to a user.\n",
    "\n",
    "We will treat our movie rating data a little differently.  Rather than using real valued ratings of 0.5-5.0 stars, we will simply treat the data as a 1 if the movie was rated (the user interacted with the movie) and 0 if the user did not rate the move.  This {0,1} rating scale is known as implicit rating scale.  It is common when we just record the occurance of an event (e.g. the user took the time to rate the movie).\n",
    "\n",
    "Our recommender will simply try to predict which movies a user is likely to interact with. This is an  implicit, binary, positive only data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# MovieLens ratings database\n",
    "movie_ratings = \"ml-latest-small/ratings.csv\"\n",
    "\n",
    "# Initialize our random state for reproducability\n",
    "seed = 1\n",
    "\n",
    "# Number of ratings we will use to test the model\n",
    "challenge_length = 5\n",
    "\n",
    "# Fraction of the available ratings to use for testing, the remaining data is available for training.\n",
    "# By default we use 10% of the ratings for testing.\n",
    "testfraction=0.10\n",
    "\n",
    "# Fraction of the training set to use\n",
    "# By default we use all the training data available\n",
    "trainfraction=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Movie Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw = pd.read_csv(movie_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create implicit ratings\n",
    "\n",
    "These are just 1's for \"interacted with movie\" and 0's for did not interact with movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=pd.crosstab(ratings_raw[\"userId\"], ratings_raw[\"movieId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Ratings into a Test and Train Dataset\n",
    "\n",
    "We will use 10% of the ratings as a test set and the remaining 90% as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = ratings.sample(frac = testfraction, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ratings.drop(testset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [trainfraction < 1.0]:\n",
    "    trainset = trainset.sample(frac = trainfraction, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Challenge Set and Answer Key\n",
    "\n",
    "Testing the accuracy of our model means we need to compare it against known results.  The ratings in the test set provide the answers to the question \"what movies did the user interact with?\"\n",
    "\n",
    "We want to challenge our model to produce these answers.  We need to create a challenge set that holds back some of answers so we can measure how well our model predicts the known answers.\n",
    "\n",
    "MovieLens only includes users that have rated more that 20 movies. We can confirm this by counting the number of interactions for each user as the sum of user ratings (rows) in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a challenge set that only provides a subset of the known interactions.  We'll use our challenge_length parameter for this (default = 5).  For each user in the testset we will only provide the challenge_length movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userids in test set\n",
    "testset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first 10 rated movies of the first userid in the test set\n",
    "# https://stackoverflow.com/a/37958335/8928529\n",
    "testset.loc[testset.index[0]].sort_values(ascending=False)[0:10].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of set matching users with their movies\n",
    "challenge_list = []\n",
    "\n",
    "for userid in testset.index:\n",
    "    movies = testset.loc[userid].sort_values(ascending=False)[0:challenge_length].index.values.tolist()\n",
    "    for movieid in movies:\n",
    "        challenge_list.append((userid, movieid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a fake user with no ratings for all movies to pad our data structure to the original dimensions\n",
    "for movieid in testset.columns.values:\n",
    "    challenge_list.append((0, movieid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have ratings database similar to the original movie_ratings and can convert it to a data frame using the same steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_ratings = pd.DataFrame(challenge_list, columns=[\"userId\", \"movieId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeset = pd.crosstab(challenge_ratings[\"userId\"], challenge_ratings[\"movieId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the dummy user\n",
    "challengeset = challengeset.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challengeset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model using Cosine Similarity \n",
    "\n",
    "With a basic similarity-based recommender system we can directly compute our model values using the cosine similarity.  We aren't using a complex model like matrix factorization or a neural network that needs to learn it's embeddings through a training regimen.  We simply compute the parameters analytically, in this case using item-to-item cosine similarity.\n",
    "\n",
    "Cosine similarity measures the angle between our training vectors. It is the dot product of the length normalized training set vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim=cosine_similarity(trainset.T, trainset.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a numpy array.  We could use it directly to compute the scores but its easier if we turn it back into a data frame and add the original indexes so we know the similarity scores between different movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = pd.DataFrame(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.columns = trainset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.index = trainset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Score for Movies to Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = testset.dot(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = challengeset.dot(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Recommended Movie Interaction List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_row(m, id):\n",
    "    \"\"\"\n",
    "    sort the ouput of recommendation scores in score order for a given id\n",
    "    \n",
    "    the id is included in the tuple to simplify downstream reconstruction\n",
    "    \"\"\"\n",
    "    idlst = [id]*len(m.columns)\n",
    "    tuples = zip(m.columns, m.values[0], idlst)\n",
    "    return sorted(tuples, key=lambda x: (x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect some data to make sure we know what it looks like.  Keeep it in a dataframe for the structure and use a transpose to make it userid-by-movieid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.loc[1].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_row(score.loc[userid].to_frame().T, userid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = []\n",
    "\n",
    "for userid in challengeset.index:\n",
    "    topn = sort_row(score.loc[userid].to_frame().T, userid)\n",
    "    recs = recs + topn[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.DataFrame(recs, columns=[\"movieId\", \"score\", \"userId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect some of the ratings by looking at the first users recommended movies.  You can explore this dataset by going to the MovieLens site an adding the movieId value to the end of this url:\n",
    "\n",
    "https://movielens.org/movies/\n",
    "\n",
    "For example, https://movielens.org/movies/1 is the page for the movie with movieId=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs[recs[\"userId\"]==recs[\"userId\"][0]][\"movieId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw[ratings_raw[\"userId\"]==recs[\"userId\"][0]][\"movieId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the R-Precision of Recommendations\n",
    "\n",
    "R-Precision let's us measure how much of the relavent data we were able to discover.  It requires knowning a ground truth of relavent items, which we know from our test data sets.\n",
    "\n",
    "Basically, we measure the ratio of how many of the known movie interaction we were able to predict.  The recommendation of a relevant interaction needs to occur within the length of the known number of inter\n",
    "\n",
    "https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#R-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_precision(answer, cand):\n",
    "    set_answer = set(answer)\n",
    "    r = len(set_answer&set(cand[:len(set_answer)])) / len(set_answer)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprecs=[]\n",
    "\n",
    "for userid in challengeset.index:\n",
    "    rprec = get_r_precision(ratings_raw[ratings_raw[\"userId\"]==userid][\"movieId\"], recs[recs[\"userId\"]==userid][\"movieId\"])\n",
    "    rprecs.append((userid, rprec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprecs = pd.DataFrame(rprecs, columns=[\"userId\", \"rprec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprecs[\"rprec\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprecs[\"rprec\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling by reducing data footprint\n",
    "\n",
    "A good focus of HPC scaling is to avoid using resources you don't need.  The above dataframes are dense and store every byte of data.  This makes it hard to scale up the ratings matrix because we will quickly run out of RAM when we get larger and larger ratings data sets.  We are a small `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_bytes(a):\n",
    "    return a.data.nbytes + a.indptr.nbytes + a.indices.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_bytes(a):\n",
    "    return [a.data.nbytes, a.indptr.nbytes, a.indices.nbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_types(a):\n",
    "    return [a.data.dtype, a.indptr.dtype, a.indices.dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_megs(n):\n",
    "    \n",
    "    MB=n/1024/1024\n",
    "    return \"{:6.2f} MB\".format(MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_report(a, name=\"matrix\"):\n",
    "    print(\"shape of {}: {}\".format(name, a.shape))\n",
    "    print(\"nnz of {}: {}\".format(name, a.nnz))\n",
    "    print(\"sparsity of {}: {:3.4f} %\".format(name, 100*(1-a.nnz/(a.shape[0]*a.shape[1]))))\n",
    "    print(\"size of {}: {}\".format(name, in_megs(sparse_bytes(a))))\n",
    "    print(\"size of {} parts: data: {}, indptr: {}, indices: {}\".format(name, *map(in_megs, parts_bytes(a))))\n",
    "    print(\"type of {} parts: data: {}, indptr: {}, indices: {}\".format(name, *parts_types(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_csr(m):\n",
    "    \"\"\"\n",
    "    sort track score pairs from a sparse matrix by the score rather than index\n",
    "\n",
    "    used to sort the ouput of recommendation scores in score order.\n",
    "    \"\"\"\n",
    "    tuples = zip(m.indices, m.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sparse = scipy.sparse.csr_matrix(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_report(ratings_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sparse.nnz * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_sparse=scipy.sparse.csr_matrix(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_report(trainset_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sparse=cosine_similarity(trainset_sparse.T, trainset_sparse.T, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise for the reader: re-implement the notebook using sparse matrices to support larger data sets."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
